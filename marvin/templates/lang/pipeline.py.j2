'''
{# Jinja template for pipeline.py creation. #}
This file was generated from the template: $MARVIN_PATH/src/templates/pipeline.py.j2

Interaction is done mainly by marvin commands. However, for debugging purposes,
the CLI usage is as follows:

`$python3 ./{{target_path}} OPERATION -h HASH`
    - HASH (str) : Generated by marvin, available in the .marvin file inside
    your project directory. Used for bucket and KFP operations.
    - OPERATION (str) : one of:
        'compile_pipeline': compiles the pipeline
        'create_bucket': creates the bucket
        'prepare_env': prepares the environment in the cloud storage bucket
        'create_run': Creates a run in KFP
        'create_recurring_run': Creates a recurring run in KFP


*** DOCSTRING ***
{{pipeline.description}}
'''
import argparse
from datetime import datetime

import kfp
from kfp import dsl

from marvin.container_wrapper import ContainerWrapper as Container
from marvin.s3_utils import S3Utils


ARG_OPS = [
    'compile_pipeline',
    'create_bucket',
    'prepare_env',
    'create_run',
    'create_recurring_run',
]


# - ***Variables for user project*** - #
PROJECT_NAME = '{{ pipeline.pipelineName }}'

{% for env_var in pipeline.defaultParams.envVars %}
{{ env_var.key }} = '{{ env_var.value }}'
{% endfor %}

BUCKET_NAME_VAR = S3Utils.replace_invalid_bucket_name_chars('{{ pipeline.defaultParams.storageBucket }}')
BUCKET_PATH = ''

def create_container(**kwargs):
    '''
    Wraps container with env vars, forwards env vars with kwargs.
    '''

    return Container(
        setup_command='{{ pipeline.defaultParams.setupCommand }}',
        verbose=True,
        {% for env_var in pipeline.defaultParams.envVars %}
            {{ env_var.key }} = '{{ env_var.value }}',
        {% endfor %}
        BUCKET_NAME=BUCKET_NAME_VAR,
        BUCKET_PATH=BUCKET_PATH,
        **kwargs
    )


# TODO: Currentrly this uses only S3, must be adapted for later use with various types (create a masterClass)
def setup_storage_pipeline_dependencies(files=[]):
    '''
    Manually sets up the s3 objects for the first container.
    '''

    local_files = [
        file['key'] for file in files if file['value'] == 'local'
    ]

    s3 = S3Utils(
        S3_ENDPOINT,
        S3_ACCESS_KEY,
        S3_SECRET_KEY,
        bucket_name=BUCKET_NAME_VAR,
        bucket_path=BUCKET_PATH
    )

    s3.upload(local_files)

    # TODO: still needs to treat other file sources -> can even be user defined


# - ***USER DEFINED PIPELINE*** - #

# Generate funcions
{% for step in pipeline.pipelineSteps %}
def {{ make_step_function_name(step.name) }}():
    '''
    {{step.description}}
    '''

    container = create_container(
    {% if step.image %}
        image={{ string_or_var(step.image) }},
    {% endif %}
    {% for env_var in step.envVars %}
        {{ env_var.key }}='{{ env_var.value }}',
    {% endfor %}
    {% if step.gpu_spec %}
        gpu_spec={{ step.gpu_spec }},
    {% endif %}
    )
    container.name = '{{ step.name }}'

    container.file_inputs = [
    {% for file in step.fileInputs %}
        '{{ file }}',
    {% endfor %}
    {% for file in pipeline.defaultParams.globalFiles %}
        '{{ file }}',
    {% endfor %}
    ]

    container.file_outputs = [
    {% for file in step.fileOutputs %}
        '{{ file }}',
    {% endfor %}
    ]

    return container.run('{{step.entrypoint}}')
{% endfor %}


@dsl.pipeline(
    name='{{ pipeline.pipelineName }}',
    description='{{ pipeline.description }}'
)
def {{ make_pipeline_function_name(pipeline.pipelineName) }}():

    {% for step in pipeline.pipelineSteps %}
    {{ make_step_function_pointer_name(step.name) }} = {{ make_step_function_name(step.name) }}()
        {% if step.runAfter %}
    {{ make_step_function_pointer_name(step.name) }}.after({{ make_step_function_pointer_name(step.runAfter) }})
        {% endif %}
    {% endfor %}


# falta montar a main certa
if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument('-h', '--hash', help='UUID for storage_bucket/run.')
    parser.add_argument('op', type=str, choices=ARG_OPS, help='Operation')
    args = parser.parse_args()

    PROJECT_NAME = S3Utils.replace_invalid_bucket_name_chars(PROJECT_NAME)

    uuid_hash = args.hash
    date_str = datetime.now().strftime('%Y-%m-%d')

    bucket_path = f'{PROJECT_NAME}-{date_str}-{uuid_hash}'

    pipeline_file_path = f'{PROJECT_NAME}.yaml'

    {% if upload %}
    setup_storage_pipeline_dependencies(
        [
            {% for file_struct in pipeline.defaultParams.dependencies %}
            {{ file_struct }},
            {% endfor %}
        ]
    )
    {% endif %}

    {# # CERTO
    pipeline_file_path = 'pipelines.yaml' # extract it from your database
    pipeline_name = 'Your Pipeline Name'
    #}

    client = kfp.Client(host="{{pipeline.defaultParams.envVars.KFP_ENDPOINT}}")

    # OPERATIONS::
    if args.op == 'compile_pieline':  # this op requests a new uuid
        kfp.compiler.Compiler().compile(
            {{ make_pipeline_function_name(pipeline.pipelineName) }},
            pipeline_file_path
        )

    if args.op == 'create_bucket':
        S3Utils.create_bucket(BUCKET_NAME_VAR)

    exp_obj = None
    if args.op == 'prepare_env':
        setup_storage_pipeline_dependencies([
            {% for file in pipeline.dependencies %}
                file
            {% endfor %}
        ])
        exp_obj = client.create_experiment(
            name="{{pipeline.pipelineName}}",
            description="{{pipeline.description}}"
        )
        print(f"Bucket Path: '{bucket_path}'")

    if args.op == 'create_run':
        client.create_run_from_pipeline_package(
            experiment_name=bucket_path,
            pipeline_file=pipeline_file_path,
            {% if pipeline.defaultParams.runParams.__class__.__name__ == 'list' %}
                {% for param in pipeline.defaultParams.runParams %}
                    {% if param.__class__.__name__ == 'dict'%}
                        {{ param.key }}={{ string_or_var(param.value) }},
                    {% endif %}
                {% endfor %}
            {% endif %}
        )

    elif args.op == 'create_recurring_run':
        client.create_recurring_run(
            experiment_id=exp_obj.id,
            pipeline_package_path=pipeline_file_path,
            {% if pipeline.defaultParams.recurringRunParams.__class__.__name__ == 'list' %}
                {% for param in pipeline.defaultParams.recurringRunParams %}
                    {% if param.__class__.__name__ == 'dict'%}
                        {{ param.key }}={{ string_or_var(param.value) }},
                    {% endif %}
                {% endfor %}
            {% endif %}
        )