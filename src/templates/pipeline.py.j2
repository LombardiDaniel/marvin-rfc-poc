'''
Jinja template for pipeline creation.
'''

from datetime import datetime

import kfp
from kfp import dsl

from container_wrapper import ContainerWrapper as Container
from s3_utils import S3Utils


# - ***Variables for user project*** - #
PROJECT_NAME = '{{ pipeline.pipelineName }}'

S3_ENDPOINT_VAR = '{{ pipeline.defaultParams.envVars.S3_ENDPOINT }}'
S3_ACCESS_KEY_VAR = '{{ pipeline.defaultParams.envVars.S3_ACCESS_KEY }}'
S3_SECRET_KEY_VAR = '{{ pipeline.defaultParams.envVars.S3_SECRET_KEY }}'
BUCKET_NAME_VAR = S3Utils.replace_invalid_bucket_name_chars('{{ pipeline.defaultParams.storageBucket }}')
BUCKET_PATH = ''

def create_container(**kwargs):
    '''
    Wraps container with env vars, forwards env vars with kwargs.
    '''

    return Container(
        setup_command='{{ pipeline.defaultParams.setupCommand }}',
        verbose=True,
        S3_ENDPOINT=S3_ENDPOINT_VAR,
        S3_ACCESS_KEY=S3_ACCESS_KEY_VAR,
        S3_SECRET_KEY=S3_SECRET_KEY_VAR,
        BUCKET_NAME=BUCKET_NAME_VAR,
        BUCKET_PATH=BUCKET_PATH,
        **kwargs
    )


def setup_storage_pipeline_dependencies(files=[]):
    '''
    Manually sets up the s3 objects for the first container.
    '''

    s3 = S3Utils(
        S3_ENDPOINT_VAR,
        S3_ACCESS_KEY_VAR,
        S3_SECRET_KEY_VAR,
        bucket_name=BUCKET_NAME_VAR,
        bucket_path=BUCKET_PATH
    )

    s3.upload(files)


# - ***USER DEFINED PIPELINE*** - #

# Generate funcions
{% for step in pipeline.pipelineSteps %}
def {{ step.name }}_step_func():
    container = create_container(
    {% for env_var in step.envVars %}
        {{ env_var.key }}='{{ env_var.value }}',
    {% endfor %}
    )
    container.name = '{{ step.name }}'

    container.file_inputs = [
    {% for file in step.fileInputs %}
        '{{ file }}',
    {% endfor %}
    ]

    container.file_outputs = [
    {% for file in step.fileOutputs %}
        '{{ file }}',
    {% endfor %}
    ]

    return container.run('{{step.entrypoint}}')
{% endfor %}


@dsl.pipeline(
    name='{{ pipeline.pipelineName }}',
    description='{{ pipeline.description }}'
)
def {{ pipeline.pipelineName }}_func():

    {% for step in pipeline.pipelineSteps %}
    {{ step.name }}_step = {{ step.name }}_step_func()
        {% if loop.index > 0 %}
            {% if step.runAfter %}
    {{ step.name }}_step.after({{ step.runAfter }}_step)
            {% endif %}
        {% endif %}
    {% endfor %}


# falta montar a main certa
if __name__ == '__main__':
    # TODO: colocar algum tipo de if aqui, q usa sys.args, e a gnt passa na hr de compilar, se entrar no if, sobe os arquivos e o pipe, cria a run e bora

    PROJECT_NAME = S3Utils.replace_invalid_bucket_name_chars(PROJECT_NAME)

    # hash = uuid.uuid4()  # o proprio marvin passa o hash pro arquivo final -> template recebe o hash
    # TODO: s√≥ entra aqui SE tiver especificado que vai criar run?
    hash = '{{ pipeline.uuid }}'
    date_str = datetime.now().strftime('%Y-%m-%d')
    BUCKET_PATH = f'{PROJECT_NAME}-{date_str}-{hash}'

    pipeline_file_path = f'{PROJECT_NAME}.yaml'

    {% if upload %}
    setup_storage_pipeline_dependencies(
        [
            {% for file in pipeline.defaultParams.dependencies %}
            '{{file}}',
            {% endfor %}
        ]
    )
    {% endif %}

    kfp.compiler.Compiler().compile({{ pipeline.pipelineName }}_func, pipeline_file_path)
    print(BUCKET_PATH)


    {# # CERTO
    pipeline_file_path = 'pipelines.yaml' # extract it from your database
    pipeline_name = 'Your Pipeline Name'

    client = kfp.Client()
    pipeline = client.pipeline_uploads.upload_pipeline(pipeline_file_path, name=pipeline_name) #}
