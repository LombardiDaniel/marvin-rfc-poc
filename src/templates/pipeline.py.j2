'''
{# Jinja template for pipeline creation. #}
{{pipeline.description}}
'''
import argparse
from datetime import datetime

import kfp
from kfp import dsl

from marvin.container_wrapper import ContainerWrapper as Container
from marvin.s3_utils import S3Utils


ARG_OPS = [
    'compile_pipeline',
    'create_bucket',
    'prepare_env',
    'upload_pipeline',
    'create_run',
    'create_recurring_run',
]

ARGS_OPS_ENUM_DICT = {
    'compile_pipeline': 0,
    'create_bucket': 1,
    'prepare_env': 2,  # upload files to storage[0]
    'upload_pipeline': 3,
    'create_run': 4,
    'create_recurring_run': 5,
}


# - ***Variables for user project*** - #
PROJECT_NAME = '{{ pipeline.pipelineName }}'

{% for env_var in pipeline.defaultParams.envVars %}
{{ env_var.key }} = '{{ env_var.value }}'
{% endfor %}

BUCKET_NAME_VAR = S3Utils.replace_invalid_bucket_name_chars('{{ pipeline.defaultParams.storageBucket }}')
BUCKET_PATH = ''

def create_container(**kwargs):
    '''
    Wraps container with env vars, forwards env vars with kwargs.
    '''

    return Container(
        setup_command='{{ pipeline.defaultParams.setupCommand }}',
        verbose=True,
        {% for env_var in pipeline.defaultParams.envVars %}
        {{ env_var.key }} = '{{ env_var.value }}',
        {% endfor %}
        BUCKET_NAME=BUCKET_NAME_VAR,
        BUCKET_PATH=BUCKET_PATH,
        **kwargs
    )


# TODO: Currentrly this uses only S3, must be adapted for later use with various types (create a masterClass)
def setup_storage_pipeline_dependencies(files=[]):
    '''
    Manually sets up the s3 objects for the first container.
    '''

    local_files = [
        file['key'] for file in files if file['value'] == 'local'
    ]

    s3 = S3Utils(
        S3_ENDPOINT,
        S3_ACCESS_KEY,
        S3_SECRET_KEY,
        bucket_name=BUCKET_NAME_VAR,
        bucket_path=BUCKET_PATH
    )

    s3.upload(local_files)

    # TODO: still needs to treat other file sources -> can even be user defined


# - ***USER DEFINED PIPELINE*** - #

# Generate funcions
{% for step in pipeline.pipelineSteps %}
def {{ make_step_function_name(step.name) }}():
    '''
    {{step.description}}
    '''

    container = create_container(
    {% for env_var in step.envVars %}
        {{ env_var.key }}='{{ env_var.value }}',
    {% endfor %}
    )
    container.name = '{{ step.name }}'

    container.file_inputs = [
    {% for file in step.fileInputs %}
        '{{ file }}',
    {% endfor %}
    {% for file in pipeline.defaultParams.globalFiles %}
        '{{ file }}',
    {% endfor %}
    ]

    container.file_outputs = [
    {% for file in step.fileOutputs %}
        '{{ file }}',
    {% endfor %}
    ]

    return container.run('{{step.entrypoint}}')
{% endfor %}


@dsl.pipeline(
    name='{{ pipeline.pipelineName }}',
    description='{{ pipeline.description }}'
)
def {{ make_pipeline_function_name(pipeline.pipelineName) }}():

    {% for step in pipeline.pipelineSteps %}
    {{ make_step_function_pointer_name(step.name) }} = {{ make_step_function_name(step.name) }}()
        {% if step.runAfter %}
    {{ make_step_function_pointer_name(step.name) }}.after({{ make_step_function_pointer_name(step.runAfter) }})
        {% endif %}
    {% endfor %}


# falta montar a main certa
if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument('-h', '--hash', help='UUID for storage_bucket/run.')
    parser.add_argument('op', type=str, choices=ARG_OPS, help='Operation')
    args = parser.parse_args()

    PROJECT_NAME = S3Utils.replace_invalid_bucket_name_chars(PROJECT_NAME)

    # hash = uuid.uuid4()  # o proprio marvin passa o hash pro arquivo final -> template recebe o hash
    # TODO: sÃ³ entra aqui SE tiver especificado que vai criar run?
    global HASH_
    HASH_ = '{{ pipeline.uuid }}'  # precisa tirar o UUID desse arquivo, vai receber
    date_str = datetime.now().strftime('%Y-%m-%d')

    global BUCKET_PATH_
    BUCKET_PATH_ = f'{PROJECT_NAME}-{date_str}-{HASH_}'

    pipeline_file_path = f'{PROJECT_NAME}.yaml'

    {% if upload %}
    setup_storage_pipeline_dependencies(
        [
            {% for file_struct in pipeline.defaultParams.dependencies %}
            {{ file_struct }},
            {% endfor %}
        ]
    )
    {% endif %}

    {# kfp.compiler.Compiler().compile({{ make_pipeline_function_name(pipeline.pipelineName) }}, pipeline_file_path) #}

    {# # CERTO
    pipeline_file_path = 'pipelines.yaml' # extract it from your database
    pipeline_name = 'Your Pipeline Name'
    #}

    client = kfp.Client(host="{{pipeline.defaultParams.envVars.KFP_ENDPOINT}}")

    # OPERATIONS::
    op_num = ARGS_OPS_ENUM_DICT[args.op]

    if op_num >= ARG_OPS.index('compile_pieline'):
        kfp.compiler.Compiler().compile(
            {{ make_pipeline_function_name(pipeline.pipelineName) }},
            pipeline_file_path
        )

    if op_num >= ARG_OPS.index('create_bucket'):
        S3Utils.create_bucket(BUCKET_NAME_VAR)

    exp_obj = None
    if op_num >= ARG_OPS.index('prepare_env'):
        setup_storage_pipeline_dependencies()
        exp_obj = client.create_experiment(
            name="{{pipeline.pipelineName}}",
            description="{{pipeline.description}}"
        )
        print(f"Bucket Path: '{BUCKET_PATH_}'")

    if op_num >= ARG_OPS.index('create_run'):
        client.create_run_from_pipeline_package(
            experiment_name=BUCKET_PATH_,
            pipeline_file=pipeline_file_path,
            arguments={},  # TEMPORARY
        )

    elif op_num >= ARG_OPS.index('create_recurring_run'):
        # THIS IS DANGEROUS -> COMMENTED OUT AS OF NOW
        # client.create_recurring_run(
        #     experiment_id=exp_obj.id,
        #     cron_expression="{{pipeline.defaultParams.cronExpression}}",
        #     max_concurrency=int({{pipeline.defaultParams.max_concurrency}}),
        #     pipeline_package_path=pipeline_file_path
        # )
        pass
