'''
Jinja template for pipeline creation.
'''

from datetime import date

import kfp
from kfp import dsl

from container_wrapper import ContainerWrapper as Container
from s3_utils import S3Utils


# - ***Variables for user project*** - #
PROJECT_NAME = '{{ pipeline.pipelineName }}'

S3_ENDPOINT_VAR = '{{ pipeline.envVars.S3_ENDPOINT_VAR }}'
S3_ACCESS_KEY_VAR = '{{ pipeline.envVars.S3_ACCESS_KEY_VAR }}'
S3_SECRET_KEY_VAR = '{{ pipeline.envVars.S3_SECRET_KEY_VAR }}'
BUCKET_NAME_VAR = '{{ pipeline.envVars.BUCKET_NAME_VAR }}'


def create_container(**kwargs):
    '''
    Wraps container with env vars, forwards env vars with kwargs.
    '''

    return Container(
        setup_command='{{ pipeline.setupCommand }}',
        verbose=True,
        S3_ENDPOINT=S3_ENDPOINT_VAR,
        S3_ACCESS_KEY=S3_ACCESS_KEY_VAR,
        S3_SECRET_KEY=S3_SECRET_KEY_VAR,
        BUCKET_NAME=BUCKET_NAME_VAR,
        **kwargs
    )


def setup_storage_pipeline_dependencies(files=[]):
    '''
    Manually sets up the minio objects for the first container.
    '''

    minio = S3Utils(
        MINIO_ENDPOINT_VAR,
        MINIO_ACCESS_KEY_VAR,
        MINIO_SECRET_KEY_VAR,
        BUCKET_NAME_VAR,
    )

    minio.upload(files)


# - ***USER DEFINED PIPELINE*** - #

# Uploads dependencies -> colocar isso aqui na main
setup_minio_pipeline_dependencies(
    {% for file in pipeline.dependencies %}
    '{{ file }}',
    {% endfor %}
)

# Generate funcions
{% for step in pipeline.pipelineSteps %}
def {{ step.name }}():
    container = create_container(
    {% for env_var in step.envVars %}
        {{ env_var.key }}='{{ env_var.value }}',
    {% endfor %}
    )
    container.name = {{ step.name }}

    container.file_inputs = [
    {% for file in step.fileInputs %}
        '{{ file }}',
    {% endfor %}
    ]

    container.file_outputs = [
    {% for file in step.fileOutputs %}
        '{{ file }}',
    {% endfor %}
    ]

    return container.run('{{step.entrypoint}}')
{% endfor %}


@dsl.pipeline(
    name='{{ pipeline.pipelineName }}',
    description='{{ pipeline.description }}'
)
def {{ pipeline.pipelineName }}_func():

    {% for step in pipeline.steps %}
    {{ step.name }}_step = {{ step.name }}()
        {% if loop.index > 0 %}
            {% if step.runAfter is not none %}
    {{ step.name }}_step.after({{ step.runAfter }}_step)
            {% endif %}
        {% endif %}
    {% endfor %}


# falta montar a main certa
if __name__ == '__main__':
    global BUCKET_NAME_VAR

    # TODO: colocar algum tipo de if aqui, q usa sys.args, e a gnt passa na hr de compilar, se entrar no if, sobe os arquivos e o pipe, cria a run e bora
    setup_storage_pipeline_dependencies(
        [   # na vdd vai terq mudar: é pra invocar isso aqui só quando marndar rodar, nao qnd compilar
            {% for file in pipeline.params. %}  # TODO: colocar nome dos arquivos pra upload pro minio antes de rodar compilar o pipe
            '{{file}}',
            {% endfor %}
        ]
    )

    PROJECT_NAME = S3Utils.replace_invalid_bucket_name_chars(PROJECT_NAME)

    # hash = uuid.uuid4()  # o proprio marvin passa o hash pro arquivo final -> template recebe o hash
    # TODO: só entra aqui SE tiver especificado que vai criar run?
    hash = {{ pipeline.uuid }}
    date_str = datetime.now().strftime('%Y-%m-%d')
    BUCKET_PATH = f'{PROJECT_NAME}-{date_str}-{hash}'

    pipeline_file_path = f'{PROJECT_NAME}.yaml'

    kfp.compiler.Compiler().compile({{ pipeline.pipelineName }}_func, pipeline_file_path)


    {# # CERTO
    pipeline_file_path = 'pipelines.yaml' # extract it from your database
    pipeline_name = 'Your Pipeline Name'

    client = kfp.Client()
    pipeline = client.pipeline_uploads.upload_pipeline(pipeline_file_path, name=pipeline_name) #}
